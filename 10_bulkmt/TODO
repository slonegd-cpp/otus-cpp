A. (-) Вывод метрик не верный. Смотрите задание внимательнее:
В задании: метрики должны иметь значения соответственно:
• main поток - 10 строк, 6 команд, 1 блок
• log поток - 1 блок, 6 команд
• file1 поток - 1 блок, 6 команд
• file2 поток - 0 блоков, 0 команд

У вас: root@slonegseq 1 4 | bulkmt 2ferffre
bulk: 1, 2
bulk: 3, 4
Блоков:
поток для вывода в файл: 140186210797312 1
поток для вывода в файл: 140186219190016 1
поток для вывода в консоль: 140186227582720 2
Комманд:
поток для вывода в файл: 140186210797312 2
поток для вывода в файл: 140186219190016 2
поток для вывода в консоль: 140186227582720
4 main поток - 4 строк, 4 команд, 2 блок

B. (!) Параметр “2ferffre” воспринимается как “2”.
Лучше бросать исключение.

C. (-) Имена файлов не уникальны. Два процесса пишут в одни и те же файлы:
root@slonegd:~# rm *.log
root@slonegd:~# seq 1 4 | bulkmt 2 | grep "^bulk" | bulkmt 2
bulk: bulk: 1, 2, bulk: 3, 4

D. (-) При нехватке места на диске программа продолжает нарезать нулевые файлы:
root@slonegd:~# rm *.log
root@slonegd:~# seq 0 1000000000 > a
seq: write error: No space left on device
root@slonegd:~# seq 1 4 | bulkmt 2 | grep "^bulk"
bulk: 1, 2
bulk: 3, 4
root@slonegd:~# ll
total 5405964
drwx------ 1 root root 4096 Jun 12 09:32 ./
drwxr-xr-x 1 root root 4096 May 7 07:20 ../
-rw-r--r-- 1 root root 5535633408 Jun 12 09:31 a
-rw-r--r-- 1 root root 0 Jun 12 09:32 bulk1528795961_0.log 
rw-r--r-- 1 root root 0 Jun 12 09:32 bulk1528795961_1.log E.

(-) Сделайте юнит-тест на фиктивный выброс исключения “out of memory” в функции
ThreadFileExporter::toFile Что произойдет? Программа упадет?
Юнит-тесты должны позволять проверять такие исключительные ситуации.
Но код для этого придется изменить. Давайте сделаем так, чтобы программа не падала,
а в любой исключительной ситуации выводила диагностику проблемы и завершала свою работу!

F. (+) Скорость обработки миллиона строк в моем окружении 10 секунд
root@slonegd:~# time seq 1000001 2000000 | bulkmt 10 > /dev/null
real 0m10.273s
user 0m3.648s
sys 0m5.592s
root@slonegd:~# find -name "bulk*.log" | wc -l
100000
Хорошая производительность. Желательно иметь тест на производительность,
чтобы зафиксировать такую производительность перед тем, как вносить изменения.
Попробуйте написать так юнит-тест. Но это по желанию.

Код пока не смотрю, но переменную bool work; - сделал бы атомом. Подумайте почему?

Ответы:
A. Вывод метрик привёл в соответствие с заданием.
B. Добавил исключение.
C. Не думал, что 2 процесса одновременно запускать будут. Добавил id потока для уникальности.
D. Передаю теперь в основном потоке информацию об исключениях неосновных. Добавил тест.
F. Тест на производительность добавил, с разным количеством потоков.
Правда не совсем понимаю, потому что на тест это не очень похоже.
Просто информация сколько работает на конкретной машине.
Провалить тест нельзя, поскольку я не сравниваю полученные значения с чем-то.
Да и каждое новое тестирование затирает информацию о прошлом.



a. (!) Флаги лучше делать атомами. Понятно почему?
bool work;
Вот в таком цикле while (work)
Изменение этих переменных в другом потоке может и не дать эффекта в текущем потоке.
Вообще это достаточно грубая ошибка! Зачем придумали примитивы синхронизации если
можно все флажками обставить? У вас work даже не помечен как volatile.
Смотрите, например, здесь: https://goo.gl/A8QCbK

b. (!) По вашей реализации получается, что если в одном рабочем потоке произошло
исключение, то он просто перестает работать. Т.е. если только в одном потоке
произойдет исключение, то приложение будет работать, но при этом не правильно!
И никто об этом не узнает. Если бы система была выполнена по принципу или всё
или ничего, то проблему бы явно увидели, поправили и перезапустили.

c. (!) По поводу operator () Операторы должны соответствовать общепринятым
представлениям о них. Данный оператор, это оператор функционального объекта.
Понятно, что “парсер” - парсит. Ну пусть так и выглядит: parser.parse()
или parser.do_parsing() или еще как-то. Операторы могут очаровывать программиста.
Есть такое явление, когда язык - его обороты влияют на суть… Старайтесь избегать
показывать свое глубокое знание языка, там где без этого можно обойтись… Менее опытные
разработчики увидят такой подход и нагородят вам такой изощренный код,
что поддерживать его будет непросто.

d. (!) Старайтесь все взаимодействие между потоками переносить в интерфейсы,
т.е. все действия с синхронизацией прописывайте явно. А каждый поток - это активный
объект со своим интерфейсом и со своими внутренними механизмами разграничения
межпоточного доступа к данным.

e. (!) Разграничивайте использование глобальных объектов из разных потоков
(например: std::cout, std::cerr) Даже если сейчас вам это не требуется.
Помните, что записи из разных командных-потоков могут перемешиваться.
А также лучше сначала сформировать полную строку и выводить ее с std::enld

f. (!) Исключения можно сохранять и передавать в другой поток. См. std::exception_ptr

g. (-) Изучите вывод запуска с valgrind:
seq 1 4 | valgrind --tool=helgrind bulkmt 2
Обратите внимание, что у вас очень много гонок
root@slonegd:~# seq 1 4 | valgrind --tool=helgrind bulkmt 2 2>&1 | grep "data race" 
==368== Possible data race during read of size 8 at 0x63C850 by thread #3 
==368== Possible data race during read of size 8 at 0x5C6B1B0 by thread #3 
==368== Possible data race during read of size 8 at 0x5C6B1A0 by thread #3 
==368== Possible data race during read of size 8 at 0x63C858 by thread #3 
==368== Possible data race during write of size 8 at 0x5C6B1A0 by thread #3 
==368== Possible data race during write of size 8 at 0x63C858 by thread #3 
==368== Possible data race during read of size 4 at 0x5C6B190 by thread #3 
==368== Possible data race during write of size 4 at 0x5C6B190 by thread #3 
==368== Possible data race during read of size 8 at 0x63C868 by thread #3 
==368== Possible data race during write of size 8 at 0x63C868 by thread #3 
==368== Possible data race during read of size 8 at 0x63C898 by thread #3 
==368== Possible data race during read of size 8 at 0x63C8A8 by thread #3 
==368== Possible data race during write of size 8 at 0x63C8A8 by thread #3 
==368== Possible data race during write of size 4 at 0x5C6B560 by thread #1 
==368== Possible data race during write of size 8 at 0x5C6B550 by thread #1 
==368== Possible data race during write of size 1 at 0x5C6B568 by thread #1 
==368== Possible data race during read of size 8 at 0x5C6A4D0 by thread #1 
==368== Possible data race during write of size 1 at 0x5C6A570 by thread #1 
==368== Possible data race during read of size 8 at 0x5C69E38 by thread #1 
==368== Possible data race during write of size 1 at 0x5C69ED8 by thread #1 

h. (!) Присмотритесь к более менее стандартным пулам потоков, например: 
boost::asio::io_service::work 

Опционально:
i. (!) В задании было сказано добавить дополнительную нагрузку на CPU.
Желательно чтобы она задавалась некоторой настройкой или дополнительным
параметром запуска. Желательно количество потоков тоже параметризовать.
И играясь этими параметрами найти для вашей конфигурации некоторую зависимость.
Вот эту зависимость и хотелось бы увидеть. Будете это делать?


Ответы:
a. Ни разу не задумывался, с какой оптимизацией cmake компилирует. Видимо, по умолчанию
без оптимизации, потому проблемм с work и не возникало, и это вселило в меня уверенность, что
всё работает как задумано. Попробовал с оптимизацией и пошли глюки. Сделал work атомарной.
Но этого оказалось недостаточно. valgrind всё равно показывал data race. Пришлось ещё
обрамить мьютексом изменения этой переменной.

b. Хм, вообще и был задуман принцип всё или ничего. Только вот, как оказалось, он не работал.
По замечанию f переделал на std::exception_ptr и теперь этот принцип работает.

c. оператор () сделал по причине, что parser.parse() смотрелось странно, переделал.

d. Целью создания класса ThreadPool было абстрагироваться от синхронизации потоков. Вся
синхронизация основана на приватных членах класса. Публичные только методы добавления задания
в очередь и подождать завершения работы потоков. Или я не правильно понял эту рекомендацию?

e. Добавил возможность указать мьютекс для работы с cout.

f. Искал эту возможность, когда пытался пробросить исклчения, но отчего то не нашел и сделал
велосипед со строкой, который ещё и не работает. Переделал на std::exception_ptr.

g. valgrind больше не показывает ошибки с data race

i. Добавил переменную загрузки ЦПУ (количество random_shuffle). Правда, не стал задавать
количество потоков и загрузку в качестве параметра запуска. Но сделал тест benchmark_test,
который запускает процесс с разным количеством потоков и сложностью. На моём компьютере
лучшая производительность всегда при одном потоке.
1 file thread, загрузка 1000: 221ms
2 file thread, загрузка 1000: 244ms
3 file thread, загрузка 1000: 221ms
4 file thread, загрузка 1000: 246ms

1 file thread, загрузка 10000: 571ms
2 file thread, загрузка 10000: 1767ms
3 file thread, загрузка 10000: 1656ms
4 file thread, загрузка 10000: 1678ms

1 file thread, загрузка 20000: 1110ms
2 file thread, загрузка 20000: 3659ms
3 file thread, загрузка 20000: 3574ms
4 file thread, загрузка 20000: 3290ms
5 file thread, загрузка 20000: 3937ms
6 file thread, загрузка 20000: 3441ms
7 file thread, загрузка 20000: 3410ms
8 file thread, загрузка 20000: 3518ms


Надо исследовать зависимость процент загрузки CPU от количества потоков пишущих в файл
с вычислительной добавкой. А вы смотрите длительность. К стати, как вы объясните что
при распараллеливании время выполнения возрастает?

Поможет
https://stackoverflow.com/questions/63166/how-to-determine-cpu-and-memory-consumption-from-inside-a-process
https://supportcenter.checkpoint.com/supportcenter/portal?eventSubmit_doGoviewsolutiondetails=&solutionid=sk65143

Ответ:
Объяснение в неудачном выборе функции для нагрузки. При замене random_shuffle на shuffle
ситуация изменилась
1 file thread, загрузка 5000: 400ms
2 file thread, загрузка 5000: 208ms
3 file thread, загрузка 5000: 175ms
4 file thread, загрузка 5000: 145ms

1 file thread, загрузка 10000: 765ms
2 file thread, загрузка 10000: 388ms
3 file thread, загрузка 10000: 325ms
4 file thread, загрузка 10000: 272ms

1 file thread, загрузка 20000: 1499ms
2 file thread, загрузка 20000: 753ms
3 file thread, загрузка 20000: 577ms
4 file thread, загрузка 20000: 571ms
5 file thread, загрузка 20000: 510ms
6 file thread, загрузка 20000: 518ms
7 file thread, загрузка 20000: 486ms
8 file thread, загрузка 20000: 467ms

random_shuffle обычно использует std::rand, а согласно cppreference
гарантий потокобезопасности rand() нет.
Выходит у меня все потоки использовали одну функцию.
На это меня натолкнула Татьяна, когда обсуждали эту проблему в слаке.

Добавил тест с загрузкой CPU (cpu_load_test). Результат:
1 file thread, загрузка 5000: 56.4103%
2 file thread, загрузка 5000: 75%
3 file thread, загрузка 5000: 82.9268%
4 file thread, загрузка 5000: 94.8718%

1 file thread, загрузка 10000: 50%
2 file thread, загрузка 10000: 65%
3 file thread, загрузка 10000: 90.2439%
4 file thread, загрузка 10000: 97.619%

1 file thread, загрузка 20000: 32.5%
2 file thread, загрузка 20000: 56.4103%
3 file thread, загрузка 20000: 82.5%
4 file thread, загрузка 20000: 97.5%
5 file thread, загрузка 20000: 97.9592%
6 file thread, загрузка 20000: 100%
7 file thread, загрузка 20000: 100%
8 file thread, загрузка 20000: 100%


